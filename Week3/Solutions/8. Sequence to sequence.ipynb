{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEP532 인공지능 이론과 실제\n",
    "## Deep Learning Practice \n",
    "#### Prof. Ho-Jin Choi\n",
    "#### School of Computing, KAIST\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Models\n",
    "### Sequence to Sequence (seq2seq)\n",
    "\n",
    "Today we will learn a new neural network architecture, so-called sequence-to-sequence (seq2seq), for language processing (e.g. language translation, image captioning, conversational models and text summarization).\n",
    "\n",
    "This is made possible by the simple but powerful idea of the sequence to sequence network, in which two recurrent neural networks work together to transform one sequence to another. An encoder network condenses an input sequence into a single vector, and a decoder network unfolds that vector into a new sequence.\n",
    "\n",
    "![Sequence to sequence model](images/seq2seq.png)\n",
    "\n",
    "#### Encoder\n",
    "- A stack of several recurrent units (LSTM or GRU cells for better performance) where each accepts a single element of the input sequence, collects information for that element and propagates it forward.\n",
    "- In question-answering problem, the input sequence is a collection of all words from the question. Each word is represented as $x_i$ where $i$ is the order of that word.\n",
    "\n",
    "#### Context Vector\n",
    "- This is the final hidden state produced from the encoder part of the model. It is calculated using the formula above.\n",
    "- This vector aims to encapsulate the information for all input elements in order to help the decoder make accurate predictions.\n",
    "- It acts as the initial hidden state of the decoder part of the model.\n",
    "\n",
    "#### Decoder\n",
    "- A stack of several recurrent units where each predicts an output $y_t$ at a time step $t$.\n",
    "- Each recurrent unit accepts a hidden state from the previous unit and produces and output as well as its own hidden state.\n",
    "- In the question-answering problem, the output sequence is a collection of all words from the answer. Each word is represented as $y_i$ where $i$ is the order of that word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and preprocess the dataset\n",
    "\n",
    "We'll use a Korean chatbot dataset provided by https://github.com/songys/Chatbot_data. This dataset contains almost 12,000 question and answer pairs and served as the csv format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-05-10 11:59:08--  https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData%20.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 889842 (869K) [text/plain]\n",
      "Saving to: ‘chatbot.csv’\n",
      "\n",
      "chatbot.csv         100%[===================>] 868.99K  4.07MB/s    in 0.2s    \n",
      "\n",
      "2021-05-10 11:59:08 (4.07 MB/s) - ‘chatbot.csv’ saved [889842/889842]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget --output-document=\"chatbot.csv\" https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData%20.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset consists of three columns:\n",
    "- `Q`: question sentence\n",
    "- `A`: answer sentence\n",
    "- `label`: sentiment lab\n",
    "\n",
    "Among these columns, we are going to use only `Q` and `A`. Let's load this dataset using `pandas.read_csv()`. For readability purposes, we are going to rename columns from `Q` and `A` to `question` and `answer` respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡!</td>\n",
       "      <td>하루가 또 가네요.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SD카드 망가졌어</td>\n",
       "      <td>다시 새로 사는 게 마음 편해요.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SD카드 안돼</td>\n",
       "      <td>다시 새로 사는 게 마음 편해요.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SNS 맞팔 왜 안하지ㅠㅠ</td>\n",
       "      <td>잘 모르고 있을 수도 있어요.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>SNS 시간낭비인 거 아는데 매일 하는 중</td>\n",
       "      <td>시간을 정하고 해보세요.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>SNS 시간낭비인데 자꾸 보게됨</td>\n",
       "      <td>시간을 정하고 해보세요.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  question              answer\n",
       "0                   12시 땡!          하루가 또 가네요.\n",
       "1              1지망 학교 떨어졌어           위로해 드립니다.\n",
       "2             3박4일 놀러가고 싶다         여행은 언제나 좋죠.\n",
       "3          3박4일 정도 놀러가고 싶다         여행은 언제나 좋죠.\n",
       "4                  PPL 심하네          눈살이 찌푸려지죠.\n",
       "5                SD카드 망가졌어  다시 새로 사는 게 마음 편해요.\n",
       "6                  SD카드 안돼  다시 새로 사는 게 마음 편해요.\n",
       "7           SNS 맞팔 왜 안하지ㅠㅠ    잘 모르고 있을 수도 있어요.\n",
       "8  SNS 시간낭비인 거 아는데 매일 하는 중       시간을 정하고 해보세요.\n",
       "9        SNS 시간낭비인데 자꾸 보게됨       시간을 정하고 해보세요."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('chatbot.csv', usecols=['Q', 'A']).rename(columns={'Q': 'question', 'A': 'answer'})\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the dataset is loaded. Let's vectorize this dataset to feed this dataset into a neural network. To do that, we need to build a tokenizer to split a sentence into several tokens and to give indexes to each token. \n",
    "\n",
    "For Korean language, there are several methods to build tokenizers but, in this notebook, we will use the following two methods:\n",
    "- A subword tokenizer\n",
    "- A tokenizer based on part-of-speech tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the subword tokenizer\n",
    "![Subword](images/subword.png)\n",
    "\n",
    "Let's build the subword tokenizer to tokenize the given texts as several subwords and to transform the subword tokens into integer vectors. To do that, we are going to use `tensorflow_datasets.features.SubwordTextEncoder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pd.concat([df['question'], df['answer']], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "SubwordTextEncoder = tfds.deprecated.text.SubwordTextEncoder\n",
    "\n",
    "tokenizer = SubwordTextEncoder.build_from_corpus(corpus, target_vocab_size=2 ** 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_token = tokenizer.vocab_size\n",
    "end_token = tokenizer.vocab_size + 1\n",
    "\n",
    "def encode(text):\n",
    "    return [start_token] + tokenizer.encode(text) + [end_token]\n",
    "\n",
    "def decode(tokens):\n",
    "    return tokenizer.decode(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_words = tokenizer.vocab_size + 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the tokenizer based on part-of-speech tagger\n",
    "![Part-of-speech](images/part-of-speech.png)\n",
    "\n",
    "Let's build the tokenizer based on part-of-speech tagger to tokenize the given texts as several morphemes and to transform the morpheme tokens into integer vectors. To do that, we are going to use `konlpy` library which provides several part-of-speech taggers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: konlpy in /usr/local/lib/python3.8/site-packages (0.5.2)\n",
      "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.8/site-packages (from konlpy) (4.6.3)\n",
      "Requirement already satisfied: beautifulsoup4==4.6.0 in /usr/local/lib/python3.8/site-packages (from konlpy) (4.6.0)\n",
      "Requirement already satisfied: colorama in /usr/local/lib/python3.8/site-packages (from konlpy) (0.4.4)\n",
      "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.8/site-packages (from konlpy) (1.19.5)\n",
      "Requirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.8/site-packages (from konlpy) (3.10.0)\n",
      "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.8/site-packages (from konlpy) (1.2.1)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.8/site-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/site-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
      "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.8/site-packages (from tweepy>=3.7.0->konlpy) (2.25.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.26.3)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.8/site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 21.1.1 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()\n",
    "def preprocess_sentence(text):\n",
    "    return '<start> {} <end>'.format(' '.join(okt.morphs(text)))\n",
    "\n",
    "corpus = pd.concat([df['question'], df['answer']], ignore_index=True)\n",
    "corpus = corpus.apply(preprocess_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "tokenizer.fit_on_texts(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_token = tokenizer.word_index['<start>']\n",
    "end_token = tokenizer.word_index['<end>']\n",
    "\n",
    "def encode(text):\n",
    "    return [start_token] + tokenizer.texts_to_sequences([' '.join(okt.morphs(text))])[0] + [end_token]\n",
    "\n",
    "def decode(tokens):\n",
    "    return tokenizer.sequences_to_texts([tokens])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_words = len(tokenizer.word_index) + 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After building tokenizers, `encode()` and `decode()`, let's vectorize the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [encode(question) for question in df['question']]\n",
    "questions = tf.keras.preprocessing.sequence.pad_sequences(questions, padding='post')\n",
    "\n",
    "answers = [encode(answer) for answer in df['answer']]\n",
    "answers = tf.keras.preprocessing.sequence.pad_sequences(answers, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `tf.data.Dataset`'s methods, shuffle the dataset and make its batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "number_of_dataset = questions.shape[0]\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((questions, answers)).shuffle(number_of_dataset).batch(batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define seq2seq model\n",
    "\n",
    "Now, it is time to build the encoder and decoder models. Because these models are not provided by TensorFlow and Keras by default, we need to define our `tf.keras.Model` by manual using the class inheritance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Encoder` model takes an input vector and produces a context vector which summarizes all the input vector. To do that, we need the following layers:\n",
    "\n",
    "- `tf.keras.layers.Embedding`\n",
    "- `tf.keras.layers.GRU`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, units):\n",
    "        super().__init__()\n",
    "        self.units = units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.units, return_state=True)\n",
    "        \n",
    "    def call(self, encoder_input, encoder_state):\n",
    "        # encoder_input = (batch_size, length)\n",
    "        # encoder_state = (batch_size, units)\n",
    "\n",
    "        # encoder_input = (batch_size, length, embedding_dim)\n",
    "        encoder_input = self.embedding(encoder_input)\n",
    "        \n",
    "        # encoder_output = (batch_size, units)\n",
    "        # encoder_state = (batch_size, units)\n",
    "        encoder_output, encoder_state = self.gru(encoder_input, initial_state=encoder_state)\n",
    "        \n",
    "        return encoder_output, encoder_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Decoder` model takes the context vector from the `Encoder` and predicts a next word given the previous word inputs. In other words, `Decoder` model calculates this conditional probability: $ P(\\text{word}_{t + 1}|\\text{context}, \\text{word}_1, \\text{word}_2, \\dots, \\text{word}_t) $\n",
    "\n",
    "To do that, we need the following layers:\n",
    "\n",
    "- `tf.keras.layers.Embedding`\n",
    "- `tf.keras.layers.GRU`\n",
    "- `tf.keras.layers.Dense`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, units):\n",
    "        super().__init__()\n",
    "        self.units = units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.units, return_state=True)\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, decoder_input, decoder_state):\n",
    "        # decoder_input = (batch_size, 1)\n",
    "        # decoder_state = (batch_size, units)\n",
    "\n",
    "        # decoder_input = (batch_size, 1, embedding_dim)\n",
    "        decoder_input = self.embedding(decoder_input)\n",
    "\n",
    "        # decoder_output = (batch_size, units)\n",
    "        # decoder_state = (batch_size, units)\n",
    "        decoder_output, decoder_state = self.gru(decoder_input, initial_state=decoder_state)\n",
    "        \n",
    "        # decoder_output = (batch_size, vocab_size)\n",
    "        decoder_output = self.fc(decoder_output)\n",
    "        \n",
    "        return decoder_output, decoder_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once both the encoder and decoder are defined, we can initiate them like normal Python classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 256\n",
    "units = 1024\n",
    "\n",
    "encoder = Encoder(number_of_words, embedding_dim, units)\n",
    "decoder = Decoder(number_of_words, embedding_dim, units)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the loss and optimizer\n",
    "\n",
    "Let's define the loss functions and the optimizers for the seq2seq model. Here, because the input dataset consists sentences of various lengths, we need to consider that point when caclculating the loss. Otherwise, the loss will be too grater than expected. To do that, we create a `mask` matrix and discard unnecessary values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "_loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "def calculate_loss(actual, predicted):\n",
    "    mask = tf.math.logical_not(tf.math.equal(actual, 0))\n",
    "    loss = _loss(actual, predicted)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "    \n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train seq2seq model using `tf.GradientTape`\n",
    "\n",
    "In this notebook, rather than using `tf.keras.Model.fit()`, we will train the model more manaully using `tf.GradientTape`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(encoder_input, decoder_target):\n",
    "    loss = 0\n",
    "    with tf.GradientTape() as tape:\n",
    "        encoder_state = tf.zeros((batch_size, encoder.units))\n",
    "        encoder_output, encoder_state = encoder(encoder_input, encoder_state)\n",
    "        \n",
    "        decoder_state = encoder_state\n",
    "        decoder_input = tf.expand_dims([start_token] * batch_size, 1)\n",
    "        \n",
    "        for step in range(1, decoder_target.shape[1]):\n",
    "            predictions, decoder_state = decoder(decoder_input, decoder_state)\n",
    "            loss += calculate_loss(decoder_target[:, step], predictions)\n",
    "            \n",
    "            decoder_input = tf.expand_dims(decoder_target[:, step], 1)\n",
    "            \n",
    "    batch_loss = loss / int(decoder_target.shape[1])\n",
    "    \n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    \n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "782bafc9b7f940ce8d3e12a7026fa2dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69d16acdfdb14bcfbc45683b219ec43a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/92 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a171f0d3f830427aaaa2d4fca5f002c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/92 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74a1b78848a9457fb5bc4b1fedfc9b1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/92 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7494a89240f849898db0b6e9ba1474ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/92 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cec64385e9b43e0a749c27e176f5583",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/92 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccf3f404e21b41c5b5320f8726d68925",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/92 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbd028551ff942d1b084b2a90aba389f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/92 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f1424f70ef2462eb2f6efdaf792f403",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/92 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a974ff6fdff46469e8377933d2ee1ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/92 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "576f2818c5224a70ba3e40001bfc310b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/92 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "epoch_loss = tf.keras.metrics.Mean()\n",
    "with tqdm(total=epochs) as epoch_progress:\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss.reset_states()\n",
    "\n",
    "        with tqdm(total=number_of_dataset // batch_size) as batch_progress:\n",
    "            for batch, (encoder_input, decoder_target) in enumerate(dataset):\n",
    "                batch_loss = train_step(encoder_input, decoder_target)\n",
    "                epoch_loss(batch_loss)\n",
    "                \n",
    "                if (batch % 10) == 0:\n",
    "                    batch_progress.set_description(f'Epoch {epoch + 1}')\n",
    "                    batch_progress.set_postfix(Batch=batch, Loss=batch_loss.numpy())\n",
    "                batch_progress.update()\n",
    "        \n",
    "        epoch_progress.set_description(f'Epoch {epoch + 1}')\n",
    "        epoch_progress.set_postfix(Loss=epoch_loss.result().numpy())\n",
    "        epoch_progress.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's generate a response for a given sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listen(sentence):\n",
    "    encoder_input = encode(sentence)\n",
    "    encoder_input = tf.keras.preprocessing.sequence.pad_sequences([encoder_input], maxlen=questions.shape[1], padding='post')\n",
    "\n",
    "    encoder_state = tf.zeros((1, encoder.units))\n",
    "    encoder_output, encoder_state = encoder(encoder_input, encoder_state)\n",
    "\n",
    "    decoder_state = encoder_state\n",
    "    decoder_input = tf.expand_dims([start_token], 0)\n",
    "\n",
    "    predicted = []\n",
    "    for step in range(answers.shape[1]):\n",
    "        predictions, decoder_state = decoder(decoder_input, decoder_state)\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "        if predicted_id == end_token:\n",
    "            break\n",
    "\n",
    "        predicted.append(predicted_id)\n",
    "        decoder_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return decode(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'저 도 배워 보고 싶어요 .'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listen('반갑습니다')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'잘 전달 할 수 있을 거 예요 .'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listen('오늘 날씨가 좋네요')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
